{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4dd58ad5",
      "metadata": {
        "id": "4dd58ad5"
      },
      "source": [
        "# Building a Tiny Transformer Language Model using TensorFlow\n",
        "\n",
        "In this lab, you'll implement a simplified Transformer-based language model using TensorFlow and Keras. You'll work with a real-world text (from a PDF or a webpage) and build, train, and experiment with a tiny language model.\n",
        "\n",
        "**Note:** This model is intentionally small and trained on limited data. The generated text might be funny or incomplete, but the goal is to understand the building blocks of Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a341c198",
      "metadata": {
        "id": "a341c198"
      },
      "source": [
        "## 1. Setup and Import Libraries\n",
        "\n",
        "Make sure you have the required packages installed:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow numpy PyPDF2 beautifulsoup4 requests\n",
        "```\n",
        "\n",
        "Now, import the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b76fa8b5",
      "metadata": {
        "id": "b76fa8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a338c3b7",
      "metadata": {
        "id": "a338c3b7"
      },
      "source": [
        "## 2. Define Model Components\n",
        "\n",
        "We'll build the following components:\n",
        "\n",
        "- **Positional Encoding**\n",
        "- **Multi-Head Self-Attention**\n",
        "- **Feed-Forward Network**\n",
        "- **Transformer Block**\n",
        "- **Tiny Transformer Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb89c68c",
      "metadata": {
        "id": "bb89c68c"
      },
      "source": [
        "### 2.1 Positional Encoding\n",
        "\n",
        "This layer adds positional information to token embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e03a36",
      "metadata": {},
      "source": [
        "Mathematic Formula \n",
        "\n",
        "![Alternative Text](https://github.com/amiraliz93/Lab2_week1/blob/main/Pics/position%20formula.PNG)\n",
        "\n",
        "\n",
        "\n",
        "```markdown\n",
        "![Example Image](https://github.com/amiraliz93/Lab2_week1/blob/main/Pics/position%20formula.PNG)\n",
        "```\n",
        "\n",
        "```markdown\n",
        "![Example Image](https://github.com/amiraliz93/Lab2_week1/blob/main/Pics/position%20formula.PNG)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "652c0096",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95b4f1d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc846f1",
      "metadata": {
        "id": "edc846f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\N1259534\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "PositionalEncoding created successfully\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model # int, \n",
        "\n",
        "        # Create a matrix of shape (max_len, d_model) with positional encodings\n",
        "        pos_enc = np.zeros((max_len, d_model))\n",
        "        pos = np.arange(max_len)[:, np.newaxis]\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pos_enc[:, 0::2] = np.sin(pos * div_term)\n",
        "        pos_enc[:, 1::2] = np.cos(pos * div_term)\n",
        "        pos_enc = pos_enc[np.newaxis, ...]  # Shape: (1, max_len, d_model)\n",
        "        self.pos_enc = tf.cast(pos_enc, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_enc[:, :seq_len, :]\n",
        "\n",
        "# Test the PositionalEncoding layer\n",
        "sample_pe = PositionalEncoding(d_model=16, max_len=50)\n",
        "print('PositionalEncoding created successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "66b578e1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.PositionalEncoding at 0x28d731bbc10>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_pe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0f1508",
      "metadata": {
        "id": "bf0f1508"
      },
      "source": [
        "### 2.2 Multi-Head Self-Attention\n",
        "\n",
        "This layer implements multi-head self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d087e531",
      "metadata": {
        "id": "d087e531"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = layers.Dense(d_model)\n",
        "        self.wk = layers.Dense(d_model)\n",
        "        self.wv = layers.Dense(d_model)\n",
        "        self.dense = layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        # Transpose to shape: (batch_size, num_heads, seq_len, depth)\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        q = self.wq(x)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "\n",
        "        # Split heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, num_heads, seq_len, seq_len)\n",
        "        dk = tf.cast(self.depth, tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        scaled_attention = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
        "\n",
        "        # Concatenate heads\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "# Test MultiHeadSelfAttention\n",
        "sample_mha = MultiHeadSelfAttention(d_model=16, num_heads=4)\n",
        "dummy_input = tf.random.uniform((1, 10, 16))\n",
        "print('MultiHeadSelfAttention output shape:', sample_mha(dummy_input).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f57fbc",
      "metadata": {
        "id": "a7f57fbc"
      },
      "source": [
        "### 2.3 Feed-Forward Network\n",
        "\n",
        "A simple two-layer feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7394b205",
      "metadata": {
        "id": "7394b205"
      },
      "outputs": [],
      "source": [
        "class FeedForward(layers.Layer):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.dense1 = layers.Dense(d_ff, activation='relu')\n",
        "        self.dense2 = layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.dense2(self.dense1(x))\n",
        "\n",
        "# Test FeedForward\n",
        "sample_ff = FeedForward(d_model=16, d_ff=32)\n",
        "print('FeedForward output shape:', sample_ff(dummy_input).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa9a6f3",
      "metadata": {
        "id": "caa9a6f3"
      },
      "source": [
        "### 2.4 Transformer Block\n",
        "\n",
        "This block combines the multi-head self-attention and feed-forward network with residual connections and layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd82b70",
      "metadata": {
        "id": "0dd82b70"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        attn_output = self.att(x)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Test TransformerBlock\n",
        "sample_tb = TransformerBlock(d_model=16, num_heads=4, d_ff=32)\n",
        "print('TransformerBlock output shape:', sample_tb(dummy_input, training=False).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b02268",
      "metadata": {
        "id": "05b02268"
      },
      "source": [
        "### 2.5 Tiny Transformer Language Model\n",
        "\n",
        "This model stacks the embedding, positional encoding, and multiple Transformer blocks to predict the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a7622c",
      "metadata": {
        "id": "82a7622c"
      },
      "outputs": [],
      "source": [
        "class TinyTransformerLM(keras.Model):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):\n",
        "        super(TinyTransformerLM, self).__init__()\n",
        "        self.token_embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.transformer_layers = [TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "        self.dropout = layers.Dropout(0.1)\n",
        "        self.final_layer = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # x shape: (batch, seq_len)\n",
        "        x = self.token_embedding(x)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, training=training)\n",
        "        x = self.dropout(x, training=training)\n",
        "        logits = self.final_layer(x)\n",
        "        return logits\n",
        "\n",
        "# The model will be used later for training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20199fb8",
      "metadata": {
        "id": "20199fb8"
      },
      "source": [
        "## 3. Preprocessing a Real-World Text\n",
        "\n",
        "For this lab, you can extract text from a webpage. The examples below show how to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e1e6b98",
      "metadata": {
        "id": "0e1e6b98"
      },
      "source": [
        "### 3.1 Extracting Text from a Webpage\n",
        "\n",
        "We'll use **Requests** and **BeautifulSoup** to extract text from a webpage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f50c9b2",
      "metadata": {
        "id": "4f50c9b2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_webpage(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "text_data = extract_text_from_webpage(\"https://gist.githubusercontent.com/sgsinclair/f895f2b37cdee761ac08e4ed8cc83d58/raw/b675c976913a939b325f7f2ee4ab4b3b396edd35/CharlesDickens-OliverTwist.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4edbb0ad",
      "metadata": {
        "id": "4edbb0ad"
      },
      "source": [
        "### 3.2 Preprocessing and Tokenization\n",
        "\n",
        "For simplicity, we'll use word-level tokenization. In practice, you might use more advanced tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf3e8e7",
      "metadata": {
        "id": "2cf3e8e7"
      },
      "outputs": [],
      "source": [
        "# Basic preprocessing: lowercasing and splitting into words\n",
        "text_data = text_data.lower()\n",
        "tokens = text_data.split()\n",
        "\n",
        "# Build vocabulary (sorted set of words)\n",
        "vocab = sorted(set(tokens))\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "# Encode the entire text into a list of integers\n",
        "encoded_text = [word2idx[word] for word in tokens]\n",
        "\n",
        "print('Vocabulary size:', len(vocab))\n",
        "print('Sample encoded text:', encoded_text[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9db62e",
      "metadata": {
        "id": "4e9db62e"
      },
      "source": [
        "### 3.3 Creating Training Examples\n",
        "\n",
        "We'll use a sliding window approach. For a given sequence length, the model will learn to predict the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a870aa03",
      "metadata": {
        "id": "a870aa03"
      },
      "outputs": [],
      "source": [
        "sequence_length = 10  # Adjust as needed\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(encoded_text) - sequence_length):\n",
        "    inputs.append(encoded_text[i:i+sequence_length])\n",
        "    targets.append(encoded_text[i+1:i+sequence_length+1])\n",
        "\n",
        "inputs = np.array(inputs)\n",
        "targets = np.array(targets)\n",
        "\n",
        "print('Number of training samples:', inputs.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb0d18e",
      "metadata": {
        "id": "3bb0d18e"
      },
      "source": [
        "### 3.4 Creating a tf.data.Dataset\n",
        "\n",
        "Now, we create a dataset for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fc4b95",
      "metadata": {
        "id": "e1fc4b95"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "print('Dataset ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dd9d8e",
      "metadata": {
        "id": "b9dd9d8e"
      },
      "source": [
        "## 4. Training the Model\n",
        "\n",
        "Set hyperparameters, compile the model, and train it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e9b153",
      "metadata": {
        "id": "73e9b153"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 2\n",
        "max_len = sequence_length\n",
        "\n",
        "# Instantiate the model\n",
        "model = TinyTransformerLM(vocab_size, d_model, num_heads, d_ff, num_layers, max_len)\n",
        "\n",
        "# Compile the model\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=loss_object)\n",
        "\n",
        "# Train the model\n",
        "epochs = 5  # Increase if needed\n",
        "model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3d4f0a",
      "metadata": {
        "id": "2a3d4f0a"
      },
      "source": [
        "## 5. Generating Text\n",
        "\n",
        "After training, use the model to generate text from a seed phrase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "739eec90",
      "metadata": {
        "id": "739eec90"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, gen_length=100):\n",
        "    print (start_text, end=\" \")\n",
        "    # Tokenize the starting text\n",
        "    tokens = start_text.lower().split()\n",
        "    input_seq = [word2idx[word] for word in tokens if word in word2idx]\n",
        "    input_seq = tf.expand_dims(input_seq, 0)  # shape: (1, current_seq_len)\n",
        "\n",
        "    generated = tokens.copy()\n",
        "    last_word=tokens[-1]\n",
        "    for _ in range(gen_length):\n",
        "        predictions = model(input_seq, training=False)  # (1, seq_len, vocab_size)\n",
        "        last_logits = predictions[0, -1, :]\n",
        "        predicted_id = tf.random.categorical(tf.expand_dims(last_logits, 0), num_samples=1)[0, 0].numpy()\n",
        "        new_word=idx2word[predicted_id]\n",
        "        print (new_word, end=\" \") if (new_word!=last_word) else None\n",
        "        last_word=new_word\n",
        "\n",
        "        # Append the new token and trim the sequence if necessary\n",
        "        input_seq = tf.concat([input_seq, tf.expand_dims([predicted_id], 0)], axis=1)\n",
        "        if input_seq.shape[1] > sequence_length:\n",
        "            input_seq = input_seq[:, -sequence_length:]\n",
        "\n",
        "# Generate text using a seed phrase\n",
        "generated_text = generate_text(model, start_text=\"Oliver leaned his head upon his hand and\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5c6e49",
      "metadata": {
        "id": "6b5c6e49"
      },
      "source": [
        "## 6. Discussion & Experimentation\n",
        "\n",
        "- Try changing hyperparameters (like `num_layers`, `d_model`, `sequence_length`, etc.)\n",
        "- Experiment with different text data (e.g., a chapter from a public-domain book)\n",
        "- Discuss how scaling the model and dataset affects performance\n",
        "\n",
        "Happy coding and experimenting!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
